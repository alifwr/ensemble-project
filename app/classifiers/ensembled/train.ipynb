{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc790118",
   "metadata": {},
   "source": [
    "# Weighted Ensemble Training\n",
    "## Network Intrusion Detection using UNSW-NB15 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3208e3",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Ensure the UNSW-NB15 dataset files are placed in the `data/` directory:\n",
    "  - `UNSW_NB15_training-set.csv`\n",
    "  - `UNSW_NB15_testing-set.csv`\n",
    "- Install dependencies: `uv sync`\n",
    "- Run this notebook from the project root directory\n",
    "- Base classifiers (ELM, KNN, SVM) will be trained first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17271c59",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96036dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent.parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from app.utils.preprocessor import UNSWNB15Preprocessor\n",
    "from app.classifiers.elm.model import ELM\n",
    "from app.classifiers.knn.model import KNN\n",
    "from app.classifiers.svm.model import SVM\n",
    "from app.classifiers.ensembled.model import WeightedEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19256a97",
   "metadata": {},
   "source": [
    "## 2. Initialize Preprocessor and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = UNSWNB15Preprocessor()\n",
    "\n",
    "# Define paths to data files\n",
    "data_dir = project_root / 'data'\n",
    "train_path = data_dir / 'UNSW_NB15_training-set.csv'\n",
    "test_path = data_dir / 'UNSW_NB15_testing-set.csv'\n",
    "\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = preprocessor.load_data(\n",
    "    train_path=str(train_path),\n",
    "    test_path=str(test_path)\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "X_train_processed, y_train_encoded = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_processed, y_test_encoded = preprocessor.transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6e45d",
   "metadata": {},
   "source": [
    "## 3. Split Training Data for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data to create validation set for weight calculation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train_processed, y_train_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_train_encoded\n",
    ")\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'DATASET SPLIT')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'Training samples: {X_train_split.shape[0]}')\n",
    "print(f'Validation samples: {X_val.shape[0]}')\n",
    "print(f'Testing samples: {X_test_processed.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d0c30",
   "metadata": {},
   "source": [
    "## 4. Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = preprocessor.get_class_names()\n",
    "class_names = [str(c) for c in classes]\n",
    "features = preprocessor.get_feature_names()\n",
    "\n",
    "input_dim = X_train_processed.shape[1]\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'DATASET INFORMATION')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'Input dimension: {input_dim}')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Classes: {class_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24384cb",
   "metadata": {},
   "source": [
    "## 5. Train Base Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8640b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'TRAINING BASE CLASSIFIERS')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "# Initialize base classifiers\n",
    "elm_model = ELM(n_hidden=1000, activation='sigmoid', C=1.0, random_state=42)\n",
    "knn_model = KNN(n_neighbors=5, weights='uniform', metric='euclidean', batch_size=1000)\n",
    "svm_model = SVM(C=1.0, kernel='rbf', gamma='scale', max_iter=-1, random_state=42, cache_size=200)\n",
    "\n",
    "base_classifiers = [\n",
    "    ('ELM', elm_model),\n",
    "    ('KNN', knn_model),\n",
    "    ('SVM', svm_model)\n",
    "]\n",
    "\n",
    "training_times = {}\n",
    "\n",
    "# Train each base classifier\n",
    "for name, clf in base_classifiers:\n",
    "    print(f'\\n{\"-\"*60}')\n",
    "    print(f'Training {name}...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    training_times[name] = training_time\n",
    "    \n",
    "    # Validation accuracy\n",
    "    val_accuracy = clf.score(X_val, y_val)\n",
    "    \n",
    "    print(f'{name} training completed in {training_time:.2f} seconds')\n",
    "    print(f'{name} validation accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "print(f'\\n{\"-\"*60}')\n",
    "print(f'All base classifiers trained successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409154d",
   "metadata": {},
   "source": [
    "## 6. Create and Fit Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'CREATING WEIGHTED ENSEMBLE')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "# Extract classifiers from list\n",
    "classifiers = [clf for _, clf in base_classifiers]\n",
    "\n",
    "# Create ensemble with dynamic weight calculation based on validation accuracy\n",
    "ensemble = WeightedEnsemble(\n",
    "    classifiers=classifiers,\n",
    "    weight_strategy='accuracy',  # Dynamic weights based on validation accuracy\n",
    "    voting='soft'  # Soft voting using probabilities\n",
    ")\n",
    "\n",
    "# Fit ensemble (calculates weights based on validation performance)\n",
    "print(f'\\nCalculating dynamic weights based on validation accuracy...')\n",
    "ensemble.fit(X_train_split, y_train_split, validation_data=(X_val, y_val))\n",
    "\n",
    "# Display calculated weights\n",
    "print(f'\\n{\"-\"*60}')\n",
    "print(f'CALCULATED WEIGHTS')\n",
    "print(f'{\"-\"*60}')\n",
    "weights = ensemble.get_weights()\n",
    "classifier_names = ['ELM', 'KNN', 'SVM']\n",
    "for i, (clf_key, weight) in enumerate(weights.items()):\n",
    "    print(f'{classifier_names[i]:<10}: {weight:.4f}')\n",
    "print(f'{\"-\"*60}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb0d17",
   "metadata": {},
   "source": [
    "## 7. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'GENERATING PREDICTIONS')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "# Predictions on validation set\n",
    "print(f'\\nPredicting validation set...')\n",
    "y_val_pred = ensemble.predict(X_val)\n",
    "\n",
    "# Predictions on test set\n",
    "if y_test_encoded is not None:\n",
    "    print(f'Predicting test set...')\n",
    "    y_test_pred = ensemble.predict(X_test_processed)\n",
    "else:\n",
    "    raise ValueError(\"y_test_encoded is None - labels are required for evaluation\")\n",
    "\n",
    "print(f'Predictions completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cbd5d",
   "metadata": {},
   "source": [
    "## 8. Calculate Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred, average='weighted', zero_division=0)\n",
    "val_recall = recall_score(y_val, y_val_pred, average='weighted', zero_division=0)\n",
    "val_f1 = f1_score(y_val, y_val_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
    "test_precision = precision_score(y_test_encoded, y_test_pred, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_test_encoded, y_test_pred, average='weighted', zero_division=0)\n",
    "test_f1 = f1_score(y_test_encoded, y_test_pred, average='weighted', zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72f49b",
   "metadata": {},
   "source": [
    "## 9. Display Overall Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'MODEL EVALUATION')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'\\nOverall Performance Metrics:')\n",
    "print(f'{\"-\"*60}')\n",
    "print(f'{\"Metric\":<20} {\"Validation\":<20} {\"Testing\":<20}')\n",
    "print(f'{\"-\"*60}')\n",
    "print(f'{\"Accuracy\":<20} {val_accuracy:<20.4f} {test_accuracy:<20.4f}')\n",
    "print(f'{\"Precision\":<20} {val_precision:<20.4f} {test_precision:<20.4f}')\n",
    "print(f'{\"Recall\":<20} {val_recall:<20.4f} {test_recall:<20.4f}')\n",
    "print(f'{\"F1-Score\":<20} {val_f1:<20.4f} {test_f1:<20.4f}')\n",
    "print(f'{\"-\"*60}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c667f2",
   "metadata": {},
   "source": [
    "## 10. Compare with Base Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa61b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'COMPARISON WITH BASE CLASSIFIERS (Test Set)')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'\\n{\"Classifier\":<20} {\"Accuracy\":<15} {\"Precision\":<15} {\"Recall\":<15} {\"F1-Score\":<15}')\n",
    "print(f'{\"-\"*85}')\n",
    "\n",
    "# Evaluate base classifiers on test set\n",
    "for name, clf in base_classifiers:\n",
    "    y_pred = clf.predict(X_test_processed)\n",
    "    acc = accuracy_score(y_test_encoded, y_pred)\n",
    "    prec = precision_score(y_test_encoded, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test_encoded, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test_encoded, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f'{name:<20} {acc:<15.4f} {prec:<15.4f} {rec:<15.4f} {f1:<15.4f}')\n",
    "\n",
    "# Display ensemble performance\n",
    "print(f'{\"-\"*85}')\n",
    "print(f'{\"Ensemble\":<20} {test_accuracy:<15.4f} {test_precision:<15.4f} {test_recall:<15.4f} {test_f1:<15.4f}')\n",
    "print(f'{\"-\"*85}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b946fd",
   "metadata": {},
   "source": [
    "## 11. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'DETAILED CLASSIFICATION REPORT (Test Set)')\n",
    "print(f'{\"=\"*60}')\n",
    "print(classification_report(y_test_encoded, y_test_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6b8d6",
   "metadata": {},
   "source": [
    "## 12. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb990feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'CONFUSION MATRIX (Test Set)')\n",
    "print(f'{\"=\"*60}')\n",
    "cm = confusion_matrix(y_test_encoded, y_test_pred)\n",
    "print(f'\\nRows: True labels, Columns: Predicted labels')\n",
    "print(f'Classes: {class_names}\\n')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf1ec46",
   "metadata": {},
   "source": [
    "## 13. Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b01623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'PER-CLASS ACCURACY (Test Set)')\n",
    "print(f'{\"=\"*60}')\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = (y_test_encoded == i)\n",
    "    num_samples = int(np.sum(class_mask))\n",
    "    if num_samples > 0:\n",
    "        class_accuracy = float(np.sum(y_test_pred[class_mask] == i)) / num_samples\n",
    "        print(f'{class_name:<20}: {class_accuracy:.4f} ({num_samples} samples)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069121b",
   "metadata": {},
   "source": [
    "## 14. Save Report to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'SAVING REPORT')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "report_dir = project_root / 'results'\n",
    "report_dir.mkdir(exist_ok=True)\n",
    "report_file = report_dir / 'ensemble_classification_report.txt'\n",
    "\n",
    "# Calculate confusion matrix for the report\n",
    "cm = confusion_matrix(y_test_encoded, y_test_pred)\n",
    "\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'WEIGHTED ENSEMBLE CLASSIFICATION REPORT\\n')\n",
    "    f.write(f'{\"=\"*60}\\n\\n')\n",
    "    \n",
    "    f.write(f'Training Date: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}\\n\\n')\n",
    "    \n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'ENSEMBLE CONFIGURATION\\n')\n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'Base Classifiers: ELM, KNN, SVM\\n')\n",
    "    f.write(f'Weight Strategy: {ensemble.weight_strategy}\\n')\n",
    "    f.write(f'Voting: {ensemble.voting}\\n\\n')\n",
    "    \n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'CLASSIFIER WEIGHTS\\n')\n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    for i, (clf_key, weight) in enumerate(weights.items()):\n",
    "        f.write(f'{classifier_names[i]:<10}: {weight:.4f}\\n')\n",
    "    f.write('\\n')\n",
    "    \n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'BASE CLASSIFIER TRAINING TIMES\\n')\n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    for name, train_time in training_times.items():\n",
    "        f.write(f'{name:<10}: {train_time:.2f} seconds\\n')\n",
    "    f.write('\\n')\n",
    "    \n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'DATASET INFORMATION\\n')\n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'Input dimension: {input_dim}\\n')\n",
    "    f.write(f'Number of classes: {num_classes}\\n')\n",
    "    f.write(f'Training samples: {X_train_split.shape[0]}\\n')\n",
    "    f.write(f'Validation samples: {X_val.shape[0]}\\n')\n",
    "    f.write(f'Testing samples: {X_test_processed.shape[0]}\\n')\n",
    "    f.write(f'Classes: {\", \".join(class_names)}\\n\\n')\n",
    "    \n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'OVERALL PERFORMANCE METRICS\\n')\n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'{\"Metric\":<20} {\"Validation\":<20} {\"Testing\":<20}\\n')\n",
    "    f.write(f'{\"-\"*60}\\n')\n",
    "    f.write(f'{\"Accuracy\":<20} {val_accuracy:<20.4f} {test_accuracy:<20.4f}\\n')\n",
    "    f.write(f'{\"Precision\":<20} {val_precision:<20.4f} {test_precision:<20.4f}\\n')\n",
    "    f.write(f'{\"Recall\":<20} {val_recall:<20.4f} {test_recall:<20.4f}\\n')\n",
    "    f.write(f'{\"F1-Score\":<20} {val_f1:<20.4f} {test_f1:<20.4f}\\n')\n",
    "    f.write(f'{\"-\"*60}\\n\\n')\n",
    "    \n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'COMPARISON WITH BASE CLASSIFIERS (Test Set)\\n')\n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'{\"Classifier\":<20} {\"Accuracy\":<15} {\"Precision\":<15} {\"Recall\":<15} {\"F1-Score\":<15}\\n')\n",
    "    f.write(f'{\"-\"*85}\\n')\n",
    "    \n",
    "    for name, clf in base_classifiers:\n",
    "        y_pred = clf.predict(X_test_processed)\n",
    "        acc = accuracy_score(y_test_encoded, y_pred)\n",
    "        prec = precision_score(y_test_encoded, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_test_encoded, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test_encoded, y_pred, average='weighted', zero_division=0)\n",
    "        f.write(f'{name:<20} {acc:<15.4f} {prec:<15.4f} {rec:<15.4f} {f1:<15.4f}\\n')\n",
    "    \n",
    "    f.write(f'{\"-\"*85}\\n')\n",
    "    f.write(f'{\"Ensemble\":<20} {test_accuracy:<15.4f} {test_precision:<15.4f} {test_recall:<15.4f} {test_f1:<15.4f}\\n')\n",
    "    f.write(f'{\"-\"*85}\\n\\n')\n",
    "    \n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'DETAILED CLASSIFICATION REPORT (Test Set)\\n')\n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    report_str = classification_report(y_test_encoded, y_test_pred, target_names=class_names)\n",
    "    f.write(str(report_str))\n",
    "    f.write('\\n')\n",
    "    \n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'CONFUSION MATRIX (Test Set)\\n')\n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'Rows: True labels, Columns: Predicted labels\\n')\n",
    "    f.write(f'Classes: {\", \".join(class_names)}\\n\\n')\n",
    "    f.write(str(cm))\n",
    "    f.write('\\n\\n')\n",
    "    \n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    f.write(f'PER-CLASS ACCURACY (Test Set)\\n')\n",
    "    f.write(f'{\"=\"*60}\\n')\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = (y_test_encoded == i)\n",
    "        num_samples = int(np.sum(class_mask))\n",
    "        if num_samples > 0:\n",
    "            class_accuracy = float(np.sum(y_test_pred[class_mask] == i)) / num_samples\n",
    "            f.write(f'{class_name:<20}: {class_accuracy:.4f} ({num_samples} samples)\\n')\n",
    "\n",
    "print(f'\\nReport saved to: {report_file}')\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'ENSEMBLE TRAINING COMPLETE')\n",
    "print(f'{\"=\"*60}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
